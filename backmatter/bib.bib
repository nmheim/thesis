@book{MandicDaniloP2001Rnnf,
series = {Adaptive and learning systems for signal processing, communications, and control},
publisher = {John Wiley},
isbn = {0470852399},
year = {2001},
title = {Recurrent neural networks for prediction, learning algorithms, architectures, and stability},
copyright = {Adgang for KB, KUB, KU, RUC + Universitetshospitaler},
language = {eng},
address = {Chichester New York},
author = {Mandic, Danilo P},
keywords = {Signal processing.; Machine learning; Neural networks (Computer science); COMPUTERS - Neural Networks; Machine learning; Neural networks (Computer science)},
}

@book{Haykin1994,
 author = {Haykin, Simon},
 title = {Neural Networks: A Comprehensive Foundation},
 year = {1994},
 isbn = {0023527617},
 edition = {1st},
 publisher = {Prentice Hall PTR},
 address = {Upper Saddle River, NJ, USA},
} 

@book{Nielsen2015,
 author = {Michael A. Nielsen},
 title = {Neural Networks and Deep Learning},
 year = {2015},
 publisher = {Determination Press}
}

@ARTICLE{Wu1995,
author={Shaun-Inn Wu},
journal={IEEE Potentials},
title={Mirroring our thought processes [recurrent neural network and time series in forecasting]},
year={1995},
volume={14},
number={5},
pages={36-41},
abstract={To employ simple exponential smoothing in statistical forecasting, we essentially have to assume that the time series fluctuates at a gradually changing mean level. Forecasts are created on an iterative basis by weighing averages of observed values in the time series. The weights are assigned unequally with heavier weights applied to the most recent observations and exponentially declining weights to observations made far in the past. Yet, simple exponential smoothing still cannot help in making accurate predictions. One still has to monitor this forecasting system to determine whether or not the weights need to be adjusted to reduce forecasting errors. Since artificial neural network (ANN) technology provides us with weight adjusting algorithms, we propose using a special ANN architecture, a simple recurrent neural network. This network will provide a simple exponential smoothing forecasting system with an adaptive weighting scheme},
keywords={adaptive signal processing;forecasting theory;neural net architecture;recurrent neural nets;smoothing methods;statistical analysis;time series;ANN architecture;adaptive weighting;artificial neural network;exponential smoothing;forecasting errors;forecasting system;mean level;observations;recurrent neural network;statistical forecasting;thought processes;time series;weight adjusting algorithms;Artificial neural networks;Casting;Humans;Monitoring;Nerve fibers;Nervous system;Neurons;Pain;Smoothing methods;Time series analysis},
doi={10.1109/45.481511},
ISSN={0278-6648},
month={Dec},}

@article{numenta_realtime,
title = {Unsupervised real-time anomaly detection for streaming data},
journal = {Neurocomputing},
volume = {262},
number = {Supplement C},
pages = {134 - 147},
year = {2017},
note = {Online Real-Time Learning Strategies for Data Streams},
issn = {0925-2312},
author = {Subutai Ahmad and Alexander Lavin and Scott Purdy and Zuha Agha},
keywords = {Anomaly detection, Hierarchical Temporal Memory, Streaming data, Unsupervised learning, Concept drift, Benchmark dataset},
abstract = {Abstract We are seeing an enormous increase in the availability of streaming, time-series data. Largely driven by the rise of connected real-time data sources, this data presents technical challenges and opportunities. One fundamental capability for streaming analytics is to model each stream in an unsupervised fashion and detect unusual, anomalous behaviors in real-time. Early anomaly detection is valuable, yet it can be difficult to execute reliably in practice. Application constraints require systems to process data in real-time, not batches. Streaming data inherently exhibits concept drift, favoring algorithms that learn continuously. Furthermore, the massive number of independent streams in practice requires that anomaly detectors be fully automated. In this paper we propose a novel anomaly detection algorithm that meets these constraints. The technique is based on an online sequence memory algorithm called Hierarchical Temporal Memory (HTM). We also present results using the Numenta Anomaly Benchmark (NAB), a benchmark containing real-world data streams with labeled anomalies. The benchmark, the first of its kind, provides a controlled open-source environment for testing anomaly detection algorithms on streaming data. We present results and analysis for a wide range of algorithms on this benchmark, and discuss future challenges for the emerging field of streaming analytics.}
}

@article{FUNAHASHI,
title = {Approximation of dynamical systems by continuous time recurrent neural networks},
journal = {Neural Networks},
volume = {6},
number = {6},
pages = {801 - 806},
year = {1993},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80125-X},
author = {Kenichi Funahashi and Yuichi Nakamura},
keywords = {Approximation, Continuous time recurrent neural network, Dynamical
            system, Autonomous system, Trajectory, Internal state, Hidden unit,
            Continuous curve},
abstract = {Abstract In this paper, we prove that any finite time trajectory of
            a given n-dimensional dynamical system can be approximately
            realized by the internal state of the output units of a continuous
            time recurrent neural network with n output units, some hidden
            units, and an appropriate initial condition. The essential idea of
            the proof is to embed the n-dimensional dynamical system into a
            higher dimensional one which defines a recurrent neural network. As
            a corollary, we also show that any continuous curve can be
            approximated by the output of a recurrent neural network.}
}

@book{HOCHREITER,
title = {Untersuchungen zu dynamischen neuronalen Netzen},
journal = {Diploma thesis},
publisher = {TU Munich},
year = {1991},
author = {Sepp Hochreiter},
}

@article{LSTM,
author = {Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
booktitle = {Neural computation}
}

@article{ADAM,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.6980},
  archivePrefix = {arXiv},
  eprint    = {1412.6980},
  timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Fernando2003PatternRI,
  title={Pattern Recognition in a Bucket},
  author={Chrisantha Fernando and Sampsa Sojakka},
  booktitle={ECAL},
  year={2003}
}


@article{lecun1995convolutional,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995}
}

@article{uni_approx_theorem,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@article{random_bergstra,
 author = {Bergstra, James and Bengio, Yoshua},
 title = {Random Search for Hyper-parameter Optimization},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2012},
 volume = {13},
 number = {1},
 month = feb,
 year = {2012},
 issn = {1532-4435},
 pages = {281--305},
 numpages = {25},
 url = {http://dl.acm.org/citation.cfm?id=2503308.2188395},
 acmid = {2188395},
 publisher = {JMLR.org},
 keywords = {deep learning, global optimization, model selection, neural
             networks, response surface modeling},
} 

@article{razvan2012,
  author    = {Razvan Pascanu and
               Tomas Mikolov and
               Yoshua Bengio},
  title     = {On the difficulty of training recurrent neural networks},
  journal   = {CoRR},
  volume    = {abs/1211.5063},
  year      = {2012},
  url       = {http://arxiv.org/abs/1211.5063},
  archivePrefix = {arXiv},
  eprint    = {1211.5063},
  timestamp = {Wed, 07 Jun 2017 14:40:40 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1211-5063},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{jaeger2002,
    author  = {Herbert Jaeger},
    title   = {Short term memory in echo state network},
    journal = {Technical Report GMD Report 152},
    year    = {2002},
}

@article{jaeger2001,
    author = {Jaeger, Herbert},
    year = {2001},
    month = {01},
    pages = {},
    title = {The{ echo state} approach to analysing and training recurrent neural networks-with an erratum note'},
    volume = {148},
    booktitle = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report}
}

@article{narx_prediction,
author = {Diaconescu, Eugen},
year = {2008},
month = {03},
pages = {},
title = {The use of NARX neural networks to predict chaotic time series},
volume = {3},
booktitle = {WSEAS Transactions on Computer Research},
biburl = {http://www.wseas.us/e-library/transactions/research/2008/27-464.pdf}
}

@misc{boxjenkins,
  title={Statistical Models for Forecasting and Control},
  author={Box, GEP and Jenkins, GM},
  year={1970},
  publisher={Holden Day: San Francisco}
}

@article{zhangyong,
  author={Zhang Yong},
  title={New prediction of chaotic time series based on local Lyapunov exponent},
  journal={Chinese Physics B},
  volume={22},
  number={5},
  pages={050502},
  url={http://stacks.iop.org/1674-1056/22/i=5/a=050502},
  year={2013},
  abstract={A new method of predicting chaotic time series is presented based
            on a local Lyapunov exponent, by quantitatively measuring the
            exponential rate of separation or attraction of two infinitely
            close trajectories in state space. After reconstructing state space
            from one-dimensional chaotic time series, neighboring
            multiple-state vectors of the predicting point are selected to
            deduce the prediction formula by using the definition of the local
            Lyapunov exponent. Numerical simulations are carried out to test
            its effectiveness and verify its higher precision over two older
            methods. The effects of the number of referential state vectors and
            added noise on forecasting accuracy are also studied numerically.}
}

@misc{coderepo,
  author = {\texttt{torsk}},
  title = {Echo State Network Implementation},
  howpublished = {\url{https://github.com/nmheim/esn}},
  year = {2018}
}

@misc{skopt,
  author = {\texttt{skopt}},
  title = {Scikit-Optimize - Simple and efficient minimization of black-box functions.},
  howpublished = {\url{https://scikit-optimize.github.io/}},
  year = {2018},
}

@misc{es_dummies,
  author = {Gregory Trubetskoy},
  title = {Holt-Winters Forcasting for Dummies},
  howpublished = {\url{https://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting/}},
  note = {Accessed: 2018-04-26},
  year = {2016}
}

@article{winters1960forecasting,
  title={Forecasting sales by exponentially weighted moving averages},
  author={Winters, Peter R},
  journal={Management science},
  volume={6},
  number={3},
  pages={324--342},
  year={1960},
  publisher={INFORMS}
}

@article{pathak2018model,
  title={Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data:
         A Reservoir Computing Approach},
  author={Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  journal={Physical Review Letters},
  volume={120},
  number={2},
  pages={024102},
  year={2018},
  publisher={APS}
}

@article{dzamba1510atomnet,
  title={AtomNet: A Deep Convolutional Neural Network for Bioactivity
         Prediction in Structure-based Drug Discovery},
  author={Dzamba, Izharwallach Michael},
  journal={arXiv},
  year={2015}
}


@article{STMESN,
    author = {Jaeger, Herbert},
    year = {2002},
    month = {01},
    pages = {},
    title = {Short Term Memory in Echo State Networks}
}

@article{Chandola,
  author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  title = {Anomaly Detection: A Survey},
  journal = {ACM Comput. Surv.},
  issue_date = {July 2009},
  volume = {41},
  number = {3},
  month = jul,
  year = {2009},
  issn = {0360-0300},
  pages = {15:1--15:58},
  articleno = {15},
  numpages = {58},
  url = {http://doi.acm.org/10.1145/1541880.1541882},
  doi = {10.1145/1541880.1541882},
  acmid = {1541882},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {Anomaly detection, outlier detection},
} 

@book{AggarwalCharuC2017,
publisher = {Springer},
year = {2013},
title = {Outlier analysis},
copyright = {Adgang for KB, KUB, KU, RUC + Universitetshospitaler},
language = {eng},
address = {New York, NY},
author = {Aggarwal, Charu C},
keywords = {Database management.; Data mining.; Mathematical statistics.; Statistics and Computing/Statistics Programs.; Outliers (Statistics); Data mining; MATHEMATICS - Probability & Statistics - General; Data mining; Outliers (Statistics); Data Mining},
}


@article{BALDI,
title = {Neural networks and principal component analysis: Learning from examples without local minima},
journal = {Neural Networks},
volume = {2},
number = {1},
pages = {53 - 58},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90014-2},
author = {Pierre Baldi and Kurt Hornik},
keywords = {Neural networks, Principal component analysis, Learning, Back propagation}
}

@article{gori1992,
  title={On the problem of local minima in backpropagation},
  author={Gori, Marco and Tesi, Alberto},
  journal={IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  number={1},
  pages={76--86},
  year={1992},
  publisher={IEEE}
}

@book{strogatz,
  title={Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering},
  author={Strogatz, Steven H},
  year={2018},
  publisher={CRC Press}
}

@inproceedings{doya1992,
  title={Bifurcations in the learning of recurrent neural networks},
  author={Doya, Kenji},
  booktitle={Circuits and Systems, 1992. ISCAS'92. Proceedings., 1992 IEEE International Symposium on},
  volume={6},
  pages={2777--2780},
  year={1992},
  organization={IEEE}
}

@article{rumelhart1986,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533},
  year={1986},
  publisher={Nature Publishing Group}
}

@article{williams1989,
  title={Experimental analysis of the real-time recurrent learning algorithm},
  author={Williams, Ronald J and Zipser, David},
  journal={Connection Science},
  volume={1},
  number={1},
  pages={87--111},
  year={1989},
  publisher={Taylor \& Francis}
}

@inproceedings{williams1992,
  title={Training recurrent networks using the extended Kalman filter},
  author={Williams, Ronald J},
  booktitle={Neural Networks, 1992. IJCNN., International Joint Conference on},
  volume={4},
  pages={241--246},
  year={1992},
  organization={IEEE}
}

@inproceedings{sutskever2011generating,
  title={Generating text with recurrent neural networks},
  author={Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
  booktitle={Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  pages={1017--1024},
  year={2011}
}

@article{brochu2010bayesopt,
  title={A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning},
  author={Brochu, Eric and Cora, Vlad M and De Freitas, Nando},
  journal={arXiv preprint arXiv:1012.2599},
  year={2010}
}

@inproceedings{williams1996gaussian,
  title={Gaussian processes for regression},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  booktitle={Advances in neural information processing systems},
  pages={514--520},
  year={1996}
}

@incollection{rasmussen2004gaussian,
  title={Gaussian processes in machine learning},
  author={Rasmussen, Carl Edward},
  booktitle={Advanced lectures on machine learning},
  pages={63--71},
  year={2004},
  publisher={Springer}
}

@article{lorenz1963deterministic,
  title={Deterministic nonperiodic flow},
  author={Lorenz, Edward N},
  journal={Journal of the atmospheric sciences},
  volume={20},
  number={2},
  pages={130--141},
  year={1963}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}


@misc{colah_topology,
  title={Neural Networks, Manifolds, and Topology},
  author={C. Olah},
  howpublished = {\url{http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/}},
  year = {2014}
}

@article{mainichi,
  author={The Mainichi},
  title={Kuroshio Current curves for the first time in 12 years, various marine effects expected},
  howpublished = {\url{https://mainichi.jp/english/articles/20171029/p2a/00m/0na/004000c}},
  journal={The Mainichi},
  month={October},
  year={2017},
}

@misc{conv_layer_wiki,          
  author = {Wikipedia},
  title = {Convolutional neural networks},                      
  howpublished = {\url{https://en.wikipedia.org/wiki/Convolutional_neural_network}},
  year = {2018},
  note = {Accessed: 2018-07-10},
} 

@Article{Gershgorin,
    Author = {S. {Gershgorin}},
    Title = {{\"Uber die Abgrenzung der Eigenwerte einer Matrix.}},
    FJournal = {{Bulletin de l'Acad\'emie des Sciences de l'URSS. VII. S\'erie}},
    Journal = {{Bull. Acad. Sci. URSS}},
    Volume = {1931},
    Number = {6},
    Pages = {749--754},
    Year = {1931},
    Publisher = {Academy of Sciences of the Union of Soviet Socialist Republics - USSR (Akademiya Nauk SSSR)},
    Language = {Russian},
    MSC2010 = {15A18},
    Zbl = {0003.00102}
}

@article{maass2004,
  title={Computational models for generic cortical microcircuits},
  author={Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  journal={Computational neuroscience: A comprehensive approach},
  volume={18},
  pages={575},
  year={2004},
  publisher={ch}
}

@book{lanczos1950iteration,
  title={An iteration method for the solution of the eigenvalue problem of linear differential and integral operators},
  author={Lanczos, Cornelius},
  year={1950},
  publisher={United States Governm. Press Office Los Angeles, CA}
}


@book{MontgomeryRegression,
series = {Wiley series in probability and statistics ; 821},
publisher = {Wiley},
isbn = {9780470542811},
year = {2012},
title = {Introduction to linear regression analysis},
edition = {5th ed..},
language = {eng},
address = {Hoboken, NJ},
author = {Montgomery, Douglas C.},
keywords = {Regression Analysis ; Mathematics / Probability &Amp;Amp Amp Statistics / General},
lccn = {QA278.2},
}


@article{butcher2013,
title = {Reservoir computing and extreme learning machines for non-linear time-series data analysis},
journal = {Neural Networks},
volume = {38},
pages = {76 - 89},
year = {2013},
issn = {0893-6080},
author = {J.B. Butcher and D. Verstraeten and B. Schrauwen and C.R. Day and P.W. Haycock},
keywords = {Reservoir computing, Extreme learning machine, Reservoir with random static projections, Non-linearity, Short-term memory, Time-series data}
}

@article{mozerBPTT,
author = {Mozer, Michael},
year = {1995},
month = {01},
pages = {},
title = {A Focused Backpropagation Algorithm for Temporal Pattern Recognition},
volume = {3},
booktitle = {Complex Systems}
}

@article{rasmussen2014,
  title={A stratigraphic framework for abrupt climatic changes during the Last Glacial period based on three synchronized Greenland ice-core records: refining and extending the INTIMATE event stratigraphy},
  author={Rasmussen, Sune O and Bigler, Matthias and Blockley, Simon P and Blunier, Thomas and Buchardt, Susanne L and Clausen, Henrik B and Cvijanovic, Ivana and Dahl-Jensen, Dorthe and Johnsen, Sigfus J and Fischer, Hubertus and others},
  journal={Quaternary Science Reviews},
  volume={106},
  pages={14--28},
  year={2014},
  publisher={Elsevier}
}


@article{sprott,
title = {A simple chaotic delay differential equation},
journal = {Physics Letters A},
volume = {366},
number = {4},
pages = {397 - 402},
year = {2007},
issn = {0375-9601},
author = {J.C. Sprott},
keywords = {Chaos, Lyapunov exponent, Delay differential equation, Brownian motion}
}

@inproceedings{hawick2011numerical,
  title={Numerical simulation of the complex Ginzburg-Landau equation on GPUs with CUDA},
  author={Hawick, Kenneth A and Playne, Daniel P},
  booktitle={Proc. IASTED International Conference on Parallel and Distributed Computing and Networks (PDCN)},
  pages={39--45},
  year={2011}
}


@article{poulsen2018,
  title={Parameterized and resolved Southern Ocean eddy compensation},
  author={Poulsen, Mads B. and Jochum, Markus and Nuterman, Roman},
  journal={Ocean Modelling},
  volume={124},
  pages={1--15},
  year={2018},
  publisher={Elsevier}
}

@book{pedlosky2013ocean,
  title={Ocean circulation theory},
  author={Pedlosky, Joseph},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{tensorflow2016,
  title={Tensorflow: a system for large-scale machine learning.},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={OSDI},
  volume={16},
  pages={265--283},
  year={2016}
}

@article{autodiff,
  author  = {Atilim Gunes Baydin and Barak A. Pearlmutter and Alexey Andreyevich Radul and Jeffrey Mark Siskind},
  title   = {Automatic Differentiation in Machine Learning: a Survey},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {153},
  pages   = {1-43},
  url     = {http://jmlr.org/papers/v18/17-468.html}
}

@article{lukosevicius,
title = {Reservoir computing approaches to recurrent neural network training},
journal = {Computer Science Review},
volume = {3},
number = {3},
pages = {127 - 149},
year = {2009},
issn = {1574-0137},
author = {Mantas Lukosevicius and Herbert Jaeger}
}

@article {jaeger2004,
    author = {Jaeger, Herbert and Haas, Harald},
    title = {Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication},
    volume = {304},
    number = {5667},
    pages = {78--80},
    year = {2004},
    doi = {10.1126/science.1091277},
    publisher = {American Association for the Advancement of Science},
    abstract = {We present a method for learning nonlinear systems, echo state
                networks (ESNs). ESNs employ artificial recurrent neural
                networks in a way that has recently been proposed independently
                as a learning mechanism in biological brains. The learning
                method is computationally efficient and easy to use. On a
                benchmark task of predicting a chaotic time series, accuracy is
                improved by a factor of 2400 over previous techniques. The
                potential for engineering applications is illustrated by
                equalizing a communication channel, where the signal error rate
                is improved by two orders of magnitude.},
    issn = {0036-8075},
    journal = {Science}
}

@inproceedings{verstraeten2010,
  title={Memory versus non-linearity in reservoirs},
  author={Verstraeten, David and Dambre, Joni and Dutoit, Xavier and Schrauwen, Benjamin},
  booktitle={Neural Networks (IJCNN), The 2010 International Joint Conference on},
  pages={1--8},
  year={2010},
  organization={IEEE}
}

@article{farkavs2016,
  title={Computational analysis of memory capacity in echo state networks},
  author={Farka{\v{s}}, Igor and Bos{\'a}k, Radom{\'\i}r and Gergel', Peter},
  journal={Neural Networks},
  volume={83},
  pages={109--120},
  year={2016},
  publisher={Elsevier}
}


@article{siegelmann1991,
  title={Turing computability with neural nets},
  author={Siegelmann, Hava T and Sontag, Eduardo D},
  journal={Applied Mathematics Letters},
  volume={4},
  number={6},
  pages={77--80},
  year={1991},
  publisher={Elsevier}
}


@article{eckhardt1993,
  title={Local Lyapunov exponents in chaotic systems},
  author={Eckhardt, Bruno and Yao, Demin},
  journal={Physica D: Nonlinear Phenomena},
  volume={65},
  number={1-2},
  pages={100--108},
  year={1993},
  publisher={Elsevier}
}

@book{tarantola,
publisher = {Siam},
isbn = {0898715725},
year = {2005},
title = {Inverse problem theory and methods for model parameter estimation},
language = {eng},
address = {Philadelphia},
author = {Tarantola, Albert},
keywords = {inverse problems},
}

@article{huang2006,
  title={Extreme learning machine: theory and applications},
  author={Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
  journal={Neurocomputing},
  volume={70},
  number={1-3},
  pages={489--501},
  year={2006},
  publisher={Elsevier}
}

@article{fogel1997,
  title={Evolutionary algorithms in theory and practice},
  author={Fogel, David B},
  journal={Complexity},
  volume={2},
  number={4},
  pages={26--27},
  year={1997},
  publisher={Wiley Online Library}
}

@article{bohrium,
  title={Bohrium: unmodified NumPy code on CPU, GPU, and cluster},
  author={Kristensen, Mads RB and Lund, Simon AF and Blum, Troels and Skovhede, Kenneth and Vinter, Brian},
  journal={Python for High Performance and Scientific Computing (PyHPC 2013)},
  year={2013}
}


